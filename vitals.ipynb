{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13fd887",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports & Config ---\n",
    "import os, re, gc, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, accuracy_score, f1_score,\n",
    "    classification_report, confusion_matrix, precision_recall_curve\n",
    ")\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Path to your Kaggle CSV directory (MIMIC-III demo/mirror with lowercase headers).\n",
    "DATA_DIR = \"/home/sraja/.cache/kagglehub/datasets/asjad99/mimiciii/versions/1/mimic-iii-clinical-database-demo-1.4\"  # <<< EDIT THIS\n",
    "\n",
    "# Core settings\n",
    "HOURS_WINDOW = 6\n",
    "CHUNK_SIZE   = 1_000_000  # tune for your RAM\n",
    "\n",
    "# Optional feature blocks\n",
    "ENABLE_LABS = False   # set True to add LABEVENTS aggregates\n",
    "ENABLE_GCS  = False   # set True to add GCS aggregates from CHARTEVENTS via D_ITEMS\n",
    "\n",
    "# Known ITEMIDs (CareVue & MetaVision) for vitals; will be extended via D_ITEMS if present\n",
    "KNOWN_ITEM_MAP = {\n",
    "    \"hr\":       [211, 220045],\n",
    "    \"sysbp\":    [51, 220179],\n",
    "    \"diabp\":    [8368, 220180],\n",
    "    \"meanbp\":   [52, 220181],\n",
    "    \"resprate\": [618, 220210],\n",
    "    \"spo2\":     [220277],\n",
    "    \"tempc\":    [676, 223761],\n",
    "}\n",
    "\n",
    "def ensure_dir(p):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "ensure_dir(\"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913ddd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper functions ---\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def safe_read_csv(data_dir, name, usecols=None, dtype=None):\n",
    "    path = os.path.join(data_dir, name)\n",
    "    df = pd.read_csv(path, dtype=dtype, low_memory=False)\n",
    "    df = normalize_cols(df)\n",
    "    if usecols:\n",
    "        df = df[[c for c in [u.lower() for u in usecols] if c in df.columns]]\n",
    "    return df\n",
    "\n",
    "def is_stroke_icd9(code: str) -> bool:\n",
    "    if code is None: return False\n",
    "    c = re.sub(r\"[^0-9]\", \"\", str(code))\n",
    "    if c.startswith((\"430\", \"431\", \"432\")):  # hemorrhagic incl. SAH\n",
    "        return True\n",
    "    if len(c) >= 3 and (c.startswith(\"433\") or c.startswith(\"434\")):\n",
    "        return c.endswith(\"1\")              # infarction present\n",
    "    return False\n",
    "\n",
    "def build_item_map(d_items_df: pd.DataFrame | None, base_map: dict) -> dict:\n",
    "    item_map = {k: set(v) for k, v in base_map.items()}\n",
    "    if d_items_df is None or \"label\" not in d_items_df.columns:\n",
    "        return {k: sorted(v) for k, v in item_map.items()}\n",
    "    di = d_items_df.copy()\n",
    "    di[\"label_l\"] = di[\"label\"].str.lower()\n",
    "\n",
    "    ITEM_KEYWORDS = {\n",
    "        \"hr\":       [\"heart rate\"],\n",
    "        \"sysbp\":    [\"non invasive systolic\", \"systolic blood pressure\"],\n",
    "        \"diabp\":    [\"non invasive diastolic\", \"diastolic blood pressure\"],\n",
    "        \"meanbp\":   [\"non invasive mean\", \"mean blood pressure\"],\n",
    "        \"resprate\": [\"respiratory rate\"],\n",
    "        \"spo2\":     [\"spo2\", \"oxygen saturation\"],\n",
    "        \"tempc\":    [\"temperature c\", \"temperature celsius\", \"temperature f\"],\n",
    "    }\n",
    "    for var, kws in ITEM_KEYWORDS.items():\n",
    "        for kw in kws:\n",
    "            m = di[di[\"label_l\"].str.contains(kw, na=False)]\n",
    "            for iid in m[\"itemid\"].tolist():\n",
    "                item_map[var].add(int(iid))\n",
    "    return {k: sorted(v) for k, v in item_map.items()}\n",
    "\n",
    "def normalize_temp_to_c(values, uoms):\n",
    "    vals = values.copy()\n",
    "    if uoms is None: return vals\n",
    "    mask_f = uoms.astype(str).str.contains(\"f\", case=False, na=False)\n",
    "    vals.loc[mask_f] = (vals.loc[mask_f] - 32.0) * (5.0/9.0)\n",
    "    return vals\n",
    "\n",
    "def aggregate_first6h_vitals(vitals_chunk, icu_df, item_map):\n",
    "    merged = vitals_chunk.merge(\n",
    "        icu_df[[\"icustay_id\",\"hadm_id\",\"intime\"]],\n",
    "        on=\"icustay_id\",\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"\", \"_icu\")\n",
    "    )\n",
    "    # Ensure single hadm_id\n",
    "    if \"hadm_id\" not in merged.columns:\n",
    "        if \"hadm_id_icu\" in merged.columns:\n",
    "            merged[\"hadm_id\"] = merged[\"hadm_id_icu\"]\n",
    "        elif \"hadm_id_x\" in merged.columns or \"hadm_id_y\" in merged.columns:\n",
    "            merged[\"hadm_id\"] = merged.get(\"hadm_id_x\", merged.get(\"hadm_id_y\"))\n",
    "\n",
    "    merged[\"charttime\"] = pd.to_datetime(merged[\"charttime\"])\n",
    "    merged[\"intime\"]    = pd.to_datetime(merged[\"intime\"])\n",
    "    merged[\"hours_since_intime\"] = (merged[\"charttime\"] - merged[\"intime\"]).dt.total_seconds()/3600.0\n",
    "    merged = merged[(merged[\"hours_since_intime\"] >= 0) & (merged[\"hours_since_intime\"] <= HOURS_WINDOW)]\n",
    "\n",
    "    # Map ITEMID → item\n",
    "    inv = {}\n",
    "    for name, ids in item_map.items():\n",
    "        for iid in ids:\n",
    "            inv[int(iid)] = name\n",
    "    merged[\"item\"] = merged[\"itemid\"].map(inv)\n",
    "\n",
    "    merged = merged.dropna(subset=[\"item\",\"valuenum\"]).copy()\n",
    "    merged[\"valuenum\"] = pd.to_numeric(merged[\"valuenum\"], errors=\"coerce\")\n",
    "    merged = merged.dropna(subset=[\"valuenum\"])\n",
    "\n",
    "    if \"valueuom\" in merged.columns:\n",
    "        tmask = merged[\"item\"].eq(\"tempc\")\n",
    "        if tmask.any():\n",
    "            merged.loc[tmask, \"valuenum\"] = normalize_temp_to_c(merged.loc[tmask, \"valuenum\"], merged.loc[tmask, \"valueuom\"])\n",
    "\n",
    "    def agg_one(df):\n",
    "        df = df.sort_values(\"charttime\")\n",
    "        vals = df[\"valuenum\"].values.astype(float)\n",
    "        times = df[\"hours_since_intime\"].values\n",
    "        res = {\n",
    "            \"mean\": float(np.mean(vals)),\n",
    "            \"min\":  float(np.min(vals)),\n",
    "            \"max\":  float(np.max(vals)),\n",
    "            \"std\":  float(np.std(vals)) if vals.size > 1 else 0.0,\n",
    "            \"last\": float(vals[-1]),\n",
    "            \"slope\": 0.0\n",
    "        }\n",
    "        if vals.size >= 2 and (times[-1]-times[0]) > 0:\n",
    "            res[\"slope\"] = float((vals[-1] - vals[0]) / (times[-1] - times[0]))\n",
    "        return pd.Series(res)\n",
    "\n",
    "    if merged.empty:\n",
    "        return pd.DataFrame(columns=[\"hadm_id\"])\n",
    "\n",
    "    agg = merged.groupby([\"hadm_id\",\"item\"], as_index=True).apply(agg_one)\n",
    "    wide = agg.unstack(\"item\")\n",
    "    wide.columns = [f\"{stat}_{item}\" for stat, item in wide.columns]\n",
    "    wide = wide.reset_index()\n",
    "    return wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c3dc4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ADMISSIONS, ICUSTAYS, DIAGNOSES_ICD, D_ITEMS ...\n",
      "Labels: {False: 123, True: 6}\n",
      "Item map (first few): {'hr': [211, 3494, 220045, 220046, 220047], 'sysbp': [51, 220179], 'diabp': [8368, 220180], 'meanbp': [52, 220181], 'resprate': [618, 619, 220210, 224688, 224689, '...'], 'spo2': [646, 5820, 6719, 8554, 220277, '...'], 'tempc': [676, 677, 678, 679, 223761, '...']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Load base tables (lowercase columns) ---\n",
    "print(\"Loading ADMISSIONS, ICUSTAYS, DIAGNOSES_ICD, D_ITEMS ...\")\n",
    "adm  = safe_read_csv(DATA_DIR, \"ADMISSIONS.csv\",   usecols=[\"subject_id\",\"hadm_id\"])\n",
    "icu  = safe_read_csv(DATA_DIR, \"ICUSTAYS.csv\",     usecols=[\"icustay_id\",\"hadm_id\",\"intime\"])\n",
    "diag = safe_read_csv(DATA_DIR, \"DIAGNOSES_ICD.csv\",usecols=[\"subject_id\",\"hadm_id\",\"icd9_code\"])\n",
    "\n",
    "# optional D_ITEMS\n",
    "try:\n",
    "    d_items = safe_read_csv(DATA_DIR, \"D_ITEMS.csv\", usecols=[\"itemid\",\"label\",\"dbsource\"])\n",
    "except Exception:\n",
    "    d_items = None\n",
    "\n",
    "# stroke labels by hadm_id\n",
    "diag[\"label\"] = diag[\"icd9_code\"].apply(is_stroke_icd9)\n",
    "labels = diag.groupby(\"hadm_id\", as_index=False)[\"label\"].max()\n",
    "print(\"Labels:\", labels[\"label\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Build item map (extend known ids using D_ITEMS labels if available)\n",
    "item_map = build_item_map(d_items, KNOWN_ITEM_MAP)\n",
    "all_itemids = sorted({iid for ids in item_map.values() for iid in ids})\n",
    "print(\"Item map (first few):\", {k: (list(v)[:5] + (['...'] if len(v) > 5 else [])) for k, v in item_map.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b478988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning CHARTEVENTS.csv in chunks ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10695/77953074.py:9: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(chunk_iter, 1):\n",
      "/tmp/ipykernel_10695/2696766035.py:107: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  agg = merged.groupby([\"hadm_id\",\"item\"], as_index=True).apply(agg_one)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vitals wide shape: (125, 43)\n"
     ]
    }
   ],
   "source": [
    "# --- Extract vitals from CHARTEVENTS (chunked) ---\n",
    "chartevents_path = os.path.join(DATA_DIR, \"CHARTEVENTS.csv\")\n",
    "usecols = [\"row_id\",\"subject_id\",\"hadm_id\",\"icustay_id\",\"charttime\",\"itemid\",\"valuenum\",\"valueuom\"]\n",
    "\n",
    "features_wide_list = []\n",
    "print(\"Scanning CHARTEVENTS.csv in chunks ...\")\n",
    "chunk_iter = pd.read_csv(chartevents_path, usecols=usecols, chunksize=CHUNK_SIZE,\n",
    "                         dtype={\"itemid\":\"int32\",\"icustay_id\":\"float64\"}, low_memory=True)\n",
    "for i, chunk in enumerate(chunk_iter, 1):\n",
    "    chunk = normalize_cols(chunk)\n",
    "    chunk = chunk[chunk[\"itemid\"].isin(all_itemids)]\n",
    "    chunk = chunk.dropna(subset=[\"icustay_id\",\"hadm_id\"])\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "    chunk[\"icustay_id\"] = chunk[\"icustay_id\"].astype(int)\n",
    "\n",
    "    wide = aggregate_first6h_vitals(chunk, icu, item_map)\n",
    "    if not wide.empty:\n",
    "        features_wide_list.append(wide)\n",
    "\n",
    "    del chunk, wide\n",
    "    if i % 5 == 0:\n",
    "        print(f\" processed {i} chunks ...\")\n",
    "        gc.collect()\n",
    "\n",
    "if not features_wide_list:\n",
    "    raise RuntimeError(\"No vitals found for selected ITEMIDs; check DATA_DIR and item_map.\")\n",
    "features_wide = pd.concat(features_wide_list, ignore_index=True).sort_values(\"hadm_id\").drop_duplicates(subset=[\"hadm_id\"], keep=\"last\")\n",
    "print(\"Vitals wide shape:\", features_wide.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec6dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Optional) Add LABEVENTS aggregates in first 6h ---\n",
    "if ENABLE_LABS:\n",
    "    def safe_read_csv_lower(path, usecols=None, dtype=None):\n",
    "        df = pd.read_csv(path, dtype=dtype, low_memory=False)\n",
    "        df = normalize_cols(df)\n",
    "        if usecols:\n",
    "            keep = [c for c in usecols if c in df.columns]\n",
    "            df = df[keep]\n",
    "        return df\n",
    "\n",
    "    labs_path = os.path.join(DATA_DIR, \"LABEVENTS.csv\")\n",
    "    if os.path.exists(labs_path):\n",
    "        labs = safe_read_csv_lower(labs_path, usecols=[\"subject_id\",\"hadm_id\",\"itemid\",\"charttime\",\"valuenum\",\"valueuom\"])\n",
    "        labs = labs.dropna(subset=[\"hadm_id\",\"valuenum\"]).copy()\n",
    "        # Join ICU intime (by hadm_id) to compute window\n",
    "        icu_unique = icu[[\"hadm_id\",\"intime\"]].drop_duplicates(\"hadm_id\")\n",
    "        labs = labs.merge(icu_unique, on=\"hadm_id\", how=\"left\")\n",
    "        labs[\"charttime\"] = pd.to_datetime(labs[\"charttime\"])\n",
    "        labs[\"intime\"]    = pd.to_datetime(labs[\"intime\"])\n",
    "        labs[\"h\"] = (labs[\"charttime\"] - labs[\"intime\"]).dt.total_seconds()/3600.0\n",
    "        labs = labs[(labs[\"h\"]>=0) & (labs[\"h\"]<=HOURS_WINDOW)]\n",
    "        labs[\"valuenum\"] = pd.to_numeric(labs[\"valuenum\"], errors=\"coerce\")\n",
    "        labs = labs.dropna(subset=[\"valuenum\"])\n",
    "\n",
    "        def agg_lab(df):\n",
    "            df = df.sort_values(\"charttime\")\n",
    "            v = df[\"valuenum\"].values\n",
    "            t = df[\"h\"].values\n",
    "            out = {\n",
    "                \"lab_mean\": float(np.mean(v)),\n",
    "                \"lab_min\":  float(np.min(v)),\n",
    "                \"lab_max\":  float(np.max(v)),\n",
    "                \"lab_std\":  float(np.std(v)) if v.size > 1 else 0.0,\n",
    "                \"lab_last\": float(v[-1]),\n",
    "                \"lab_slope\": 0.0\n",
    "            }\n",
    "            if v.size >= 2 and (t[-1]-t[0]) > 0:\n",
    "                out[\"lab_slope\"] = float((v[-1]-v[0])/(t[-1]-t[0]))\n",
    "            return pd.Series(out)\n",
    "\n",
    "        lab_wide = labs.groupby([\"hadm_id\",\"itemid\"], as_index=True).apply(agg_lab).unstack(\"itemid\")\n",
    "        lab_wide.columns = [f\"{stat}_lab_{itemid}\" for stat, itemid in lab_wide.columns]\n",
    "        lab_wide = lab_wide.reset_index()\n",
    "        features_wide = features_wide.merge(lab_wide, on=\"hadm_id\", how=\"left\")\n",
    "        print(\"After labs →\", features_wide.shape)\n",
    "    else:\n",
    "        print(\"LABEVENTS.csv not found — skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab08614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Optional) Add GCS features from CHARTEVENTS via D_ITEMS ---\n",
    "if ENABLE_GCS:\n",
    "    d_items_path = os.path.join(DATA_DIR, \"D_ITEMS.csv\")\n",
    "    if os.path.exists(d_items_path):\n",
    "        di = pd.read_csv(d_items_path, low_memory=False)\n",
    "        di = normalize_cols(di)\n",
    "        di[\"label_l\"] = di[\"label\"].str.lower()\n",
    "\n",
    "        def find_ids(kws):\n",
    "            m = di[di[\"label_l\"].str.contains(\"|\".join([re.escape(k) for k in kws]), na=False)]\n",
    "            return sorted(set(m[\"itemid\"].astype(int).tolist()))\n",
    "\n",
    "        ids_gcs_total = find_ids([\"glasgow coma scale total\",\"gcs total\",\"gcs - total\"])\n",
    "        ids_gcs_eye   = find_ids([\"glasgow coma scale eye\",\"gcs eye\",\"gcs - eye opening\"])\n",
    "        ids_gcs_verbal= find_ids([\"glasgow coma scale verbal\",\"gcs verbal\",\"gcs - verbal response\"])\n",
    "        ids_gcs_motor = find_ids([\"glasgow coma scale motor\",\"gcs motor\",\"gcs - motor response\"])\n",
    "\n",
    "        gcs_ids_all = set(ids_gcs_total + ids_gcs_eye + ids_gcs_verbal + ids_gcs_motor)\n",
    "        usecols_ce = [\"subject_id\",\"hadm_id\",\"icustay_id\",\"charttime\",\"itemid\",\"valuenum\"]\n",
    "        gcs_parts = []\n",
    "\n",
    "        chunk_iter = pd.read_csv(chartevents_path, usecols=usecols_ce, chunksize=CHUNK_SIZE, low_memory=True)\n",
    "        for ch in chunk_iter:\n",
    "            ch = normalize_cols(ch)\n",
    "            ch = ch[ch[\"itemid\"].isin(gcs_ids_all)]\n",
    "            if ch.empty: continue\n",
    "            ch = ch.merge(icu[[\"icustay_id\",\"hadm_id\",\"intime\"]], on=\"icustay_id\", how=\"inner\")\n",
    "            ch[\"charttime\"] = pd.to_datetime(ch[\"charttime\"])\n",
    "            ch[\"intime\"]    = pd.to_datetime(ch[\"intime\"])\n",
    "            ch[\"h\"] = (ch[\"charttime\"] - ch[\"intime\"]).dt.total_seconds()/3600.0\n",
    "            ch = ch[(ch[\"h\"]>=0) & (ch[\"h\"]<=HOURS_WINDOW)]\n",
    "            ch[\"valuenum\"] = pd.to_numeric(ch[\"valuenum\"], errors=\"coerce\")\n",
    "            ch = ch.dropna(subset=[\"valuenum\"])\n",
    "            gcs_parts.append(ch)\n",
    "\n",
    "        if gcs_parts:\n",
    "            gcs_long = pd.concat(gcs_parts, ignore_index=True)\n",
    "\n",
    "            def comp_of(i):\n",
    "                if i in ids_gcs_total: return \"gcs_total\"\n",
    "                if i in ids_gcs_eye:   return \"gcs_eye\"\n",
    "                if i in ids_gcs_verbal:return \"gcs_verbal\"\n",
    "                if i in ids_gcs_motor: return \"gcs_motor\"\n",
    "                return None\n",
    "\n",
    "            gcs_long[\"comp\"] = gcs_long[\"itemid\"].map(comp_of)\n",
    "            gcs_long = gcs_long.dropna(subset=[\"comp\"])\n",
    "\n",
    "            def agg_gcs(df):\n",
    "                df = df.sort_values(\"charttime\")\n",
    "                v = df[\"valuenum\"].values\n",
    "                return pd.Series({\n",
    "                    \"mean\": float(np.mean(v)),\n",
    "                    \"min\":  float(np.min(v)),\n",
    "                    \"max\":  float(np.max(v)),\n",
    "                    \"last\": float(v[-1]),\n",
    "                })\n",
    "\n",
    "            gcs_wide = gcs_long.groupby([\"hadm_id\",\"comp\"]).apply(agg_gcs).unstack(\"comp\")\n",
    "            gcs_wide.columns = [f\"gcs_{stat}_{comp}\" for stat, comp in gcs_wide.columns]\n",
    "            gcs_wide = gcs_wide.reset_index()\n",
    "            features_wide = features_wide.merge(gcs_wide, on=\"hadm_id\", how=\"left\")\n",
    "            print(\"After GCS →\", features_wide.shape)\n",
    "        else:\n",
    "            print(\"No GCS rows found — check D_ITEMS keywords.\")\n",
    "    else:\n",
    "        print(\"D_ITEMS.csv not found — skipping GCS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873a20e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (125, 45) positives: 6\n",
      "Train balance: [95  5]\n",
      "Val   balance: [24  1]\n",
      "Class weight: {0: 0.5263157894736842, 1: 10.0}\n",
      "\n",
      "Validation metrics @0.5: {'AUROC': np.float64(1.0), 'AUPRC': np.float64(1.0), 'Accuracy': 0.96, 'F1': 0.0}\n",
      "\n",
      "Classification report @0.5:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "no-stroke(0)       0.96      1.00      0.98        24\n",
      "   stroke(1)       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.96        25\n",
      "   macro avg       0.48      0.50      0.49        25\n",
      "weighted avg       0.92      0.96      0.94        25\n",
      "\n",
      "Confusion matrix @0.5:\n",
      " [[24  0]\n",
      " [ 1  0]]\n"
     ]
    }
   ],
   "source": [
    "# --- Build ML dataset (merge labels + subject ids) ---\n",
    "dataset = features_wide.merge(labels, on=\"hadm_id\", how=\"inner\")\n",
    "dataset = dataset.merge(adm, on=\"hadm_id\", how=\"left\")  # adds subject_id\n",
    "dataset = dataset.dropna(subset=[\"label\"]).copy()\n",
    "dataset[\"label\"] = dataset[\"label\"].astype(int)\n",
    "print(\"Dataset shape:\", dataset.shape, \"positives:\", int(dataset[\"label\"].sum()))\n",
    "\n",
    "# --- Subject-level stratified split ---\n",
    "subj_lab = (\n",
    "    dataset[[\"subject_id\",\"label\"]]\n",
    "    .groupby(\"subject_id\", as_index=False)[\"label\"]\n",
    "    .max()\n",
    "    .rename(columns={\"label\":\"subj_label\"})\n",
    ")\n",
    "\n",
    "VAL_SIZE = 0.2\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SIZE, random_state=42)\n",
    "(subj_tr_idx, subj_va_idx), = sss.split(subj_lab[\"subject_id\"], subj_lab[\"subj_label\"])\n",
    "train_subjects = set(subj_lab.iloc[subj_tr_idx][\"subject_id\"])\n",
    "val_subjects   = set(subj_lab.iloc[subj_va_idx][\"subject_id\"])\n",
    "\n",
    "feature_cols = [c for c in dataset.columns if c not in (\"hadm_id\",\"subject_id\",\"label\")]\n",
    "X_train = dataset.loc[dataset[\"subject_id\"].isin(train_subjects), feature_cols].copy()\n",
    "y_train = dataset.loc[dataset[\"subject_id\"].isin(train_subjects), \"label\"].astype(int).values\n",
    "X_val   = dataset.loc[dataset[\"subject_id\"].isin(val_subjects),   feature_cols].copy()\n",
    "y_val   = dataset.loc[dataset[\"subject_id\"].isin(val_subjects),   \"label\"].astype(int).values\n",
    "\n",
    "# impute with training medians\n",
    "fill_values = X_train.median().to_dict()\n",
    "X_train = X_train.fillna(fill_values)\n",
    "X_val   = X_val.fillna(fill_values)\n",
    "\n",
    "print(\"Train balance:\", np.bincount(y_train) if len(y_train)>0 else \"[]\")\n",
    "print(\"Val   balance:\", np.bincount(y_val) if len(y_val)>0 else \"[]\")\n",
    "\n",
    "# --- Class weights for imbalance ---\n",
    "classes = np.array([0, 1])\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "class_weight = {int(c): float(w) for c, w in zip(classes, weights)}\n",
    "print(\"Class weight:\", class_weight)\n",
    "\n",
    "# --- Train RandomForest ---\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=3,\n",
    "    max_features=\"sqrt\",\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate at default 0.5 ---\n",
    "val_prob = rf.predict_proba(X_val)[:, 1]\n",
    "val_pred = (val_prob >= 0.5).astype(int)\n",
    "metrics = {\n",
    "    \"AUROC\": roc_auc_score(y_val, val_prob) if len(np.unique(y_val))==2 else float(\"nan\"),\n",
    "    \"AUPRC\": average_precision_score(y_val, val_prob) if len(np.unique(y_val))==2 else float(\"nan\"),\n",
    "    \"Accuracy\": accuracy_score(y_val, val_pred),\n",
    "    \"F1\": f1_score(y_val, val_pred, zero_division=0),\n",
    "}\n",
    "print(\"\\nValidation metrics @0.5:\", {k: (None if (isinstance(v,float) and np.isnan(v)) else round(v,4)) for k, v in metrics.items()})\n",
    "print(\"\\nClassification report @0.5:\\n\", classification_report(y_val, val_pred, target_names=[\"no-stroke(0)\",\"stroke(1)\"], zero_division=0))\n",
    "print(\"Confusion matrix @0.5:\\n\", confusion_matrix(y_val, val_pred, labels=[0,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e622c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-F1 threshold: 0.2206  (P=1.000, R=1.000)\n",
      "\n",
      "Classification report @ tuned threshold:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "no-stroke(0)       1.00      1.00      1.00        24\n",
      "   stroke(1)       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "Confusion matrix @ tuned threshold:\n",
      " [[24  0]\n",
      " [ 0  1]]\n",
      "\n",
      "Saved → models/mimiciii_vitals_rf.joblib (threshold saved)\n"
     ]
    }
   ],
   "source": [
    "# --- Threshold tuning via PR curve + save model pack ---\n",
    "prec, rec, th = precision_recall_curve(y_val, val_prob)\n",
    "thr_all = np.r_[th, 1.0]\n",
    "f1s = (2*prec*rec)/(prec+rec+1e-12)\n",
    "best_idx = int(np.nanargmax(f1s))\n",
    "tuned_thr = float(thr_all[best_idx])\n",
    "print(f\"Best-F1 threshold: {tuned_thr:.4f}  (P={prec[best_idx]:.3f}, R={rec[best_idx]:.3f})\")\n",
    "\n",
    "y_val_opt = (val_prob >= tuned_thr).astype(int)\n",
    "print(\"\\nClassification report @ tuned threshold:\\n\",\n",
    "      classification_report(y_val, y_val_opt, target_names=[\"no-stroke(0)\",\"stroke(1)\"], zero_division=0))\n",
    "print(\"Confusion matrix @ tuned threshold:\\n\", confusion_matrix(y_val, y_val_opt, labels=[0,1]))\n",
    "\n",
    "pack = {\n",
    "    \"model\": rf,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"fill_values\": fill_values,\n",
    "    \"hours_window\": HOURS_WINDOW,\n",
    "    \"item_map\": {k:list(v) for k,v in KNOWN_ITEM_MAP.items()},  # base ids (for reference)\n",
    "    \"threshold\": tuned_thr,\n",
    "    \"threshold_note\": \"best_F1_on_validation\",\n",
    "}\n",
    "dump(pack, \"models/mimiciii_vitals_rf.joblib\")\n",
    "print(\"\\nSaved → models/mimiciii_vitals_rf.joblib (threshold saved)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d5a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference helper ---\n",
    "def predict_vitals_stroke_prob(vitals_row: dict | pd.Series):\n",
    "    \"\"\"Returns (probability, predicted_label_using_saved_threshold).\"\"\"\n",
    "    pk = load(\"models/mimiciii_vitals_rf.joblib\")\n",
    "    model = pk[\"model\"]\n",
    "    cols  = pk[\"feature_cols\"]\n",
    "    fill  = pk[\"fill_values\"]\n",
    "    thr   = float(pk.get(\"threshold\", 0.5))\n",
    "    x = pd.DataFrame([vitals_row], columns=cols).fillna(fill)\n",
    "    prob = float(model.predict_proba(x)[:, 1][0])\n",
    "    pred = int(prob >= thr)\n",
    "    return prob, pred\n",
    "\n",
    "# Example (fill real values from a new admission's first-6h aggregates):\n",
    "# example_row = {c: 0.0 for c in feature_cols}\n",
    "# prob, pred = predict_vitals_stroke_prob(example_row)\n",
    "# print(prob, pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
