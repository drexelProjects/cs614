{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cc547c",
   "metadata": {},
   "source": [
    "# Emergency Stroke Response Pipeline Demo\n",
    "\n",
    "This Jupyter notebook demonstrates an end‑to‑end pipeline for processing an emergency call and patient data in the context of suspected stroke.  The pipeline includes:\n",
    "\n",
    "1. **Audio transcription (ASR)** – converting spoken words from a call recording into text using an automatic speech recognition model (e.g. OpenAI Whisper or Mozilla DeepSpeech).\n",
    "2. **Clinical summarisation** – generating a concise summary of the call transcript and other clinical notes using a biomedical language model (e.g. BioBART, ClinicalT5).\n",
    "3. **Stroke detection** – applying machine‑learning models to speech or vital signs to estimate the likelihood of a stroke.  For demonstration we include code stubs for dysarthria (slurred speech) detection and vitals‑based classification.\n",
    "4. **Timeline generation** – combining timestamped events (call, vitals, interventions) into a chronological timeline.\n",
    "\n",
    "> **Note:** This notebook provides runnable code for each stage using open‑source libraries.  However, the heavy models (ASR and LLM summarisation) require installation of external packages and may need GPU acceleration to run efficiently.  You can uncomment the installation commands to set up the environment when running this notebook.  Sample data is provided or generated synthetically where appropriate; you may replace these with your own recordings, clinical notes and vitals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1616b689",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "This section installs and imports the required libraries.  To keep the notebook light, most installation commands are commented out; remove the leading `#` to install packages in your own environment (internet access is required).\n",
    "\n",
    "- `openai‑whisper` for speech‑to‑text transcription (PyTorch).\n",
    "- `deepspeech` for an alternative TensorFlow‑based ASR engine.\n",
    "- `transformers` and `sentencepiece` for biomedical language models.\n",
    "- `torchaudio`, `librosa` and `scikit‑learn` for signal processing and machine learning.\n",
    "- `pandas`, `numpy` and `matplotlib` for data manipulation and visualisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a9cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q openai-whisper deepspeech transformers sentencepiece torchaudio librosa scikit-learn matplotlib pandas numpy\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Uncomment and run this cell after installing the above packages to import heavy libraries\n",
    "import whisper\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "# from deepspeech import Model as DeepSpeechModel\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d94cdc",
   "metadata": {},
   "source": [
    "## 2. Stage 1 – Audio Transcription\n",
    "\n",
    "In this stage we convert an emergency call recording into text using an automatic speech recognition (ASR) model.  Two popular open‑source options are:\n",
    "\n",
    "- **OpenAI Whisper** – a state‑of‑the‑art ASR model trained on a large multilingual corpus.  It is easy to use via the `openai‑whisper` Python package.  The `base`, `small`, `medium` or `large` model size can be selected depending on resource availability.\n",
    "- **Mozilla DeepSpeech / Coqui STT** – a TensorFlow‑based speech‑to‑text engine that can run offline.  Pre‑trained models and a language model (`.scorer` file) are required.\n",
    "\n",
    "Below is example code for both methods.  Replace `sample_audio.wav` with the path to your own WAV file.  Whisper automatically handles resampling and language detection; DeepSpeech expects a 16 kHz mono WAV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565026f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper transcript:\n",
      " What do I think? I think I need an ambulance. I've got a teenager who's turning purple in his face. I think he's soaked out or something.\n"
     ]
    }
   ],
   "source": [
    "#--- Whisper ASR ---\n",
    "audio_path = 'call.wav'  # Path to your call recording (WAV/MP3/OGG)\n",
    "whisper_model = whisper.load_model('base')  # Options: tiny, base, small, medium, large\n",
    "result = whisper_model.transcribe(audio_path)\n",
    "transcript = result['text']\n",
    "print('Whisper transcript:')\n",
    "print(transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70391eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DeepSpeech ASR ---\n",
    "# Ensure you have downloaded the .pbmm model file and .scorer language model file\n",
    "# Example download commands (run in a terminal):\n",
    "#   wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.pbmm\n",
    "#   wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.scorer\n",
    "\n",
    "# Uncomment to load and run DeepSpeech\n",
    "# ds_model_path = 'deepspeech-0.9.3-models.pbmm'\n",
    "# ds_scorer_path = 'deepspeech-0.9.3-models.scorer'\n",
    "# ds = DeepSpeechModel(ds_model_path)\n",
    "# ds.enableExternalScorer(ds_scorer_path)\n",
    "#\n",
    "# # Load 16 kHz WAV file (use scipy or wave module)\n",
    "# import wave\n",
    "# with wave.open('sample_audio.wav', 'rb') as fin:\n",
    "#     frames = fin.getnframes()\n",
    "#     buffer = fin.readframes(frames)\n",
    "#     sample_rate = fin.getframerate()\n",
    "#\n",
    "# assert sample_rate == 16000, 'DeepSpeech requires a 16 kHz sample rate'\n",
    "# transcript = ds.stt(buffer)\n",
    "# print('DeepSpeech transcript:')\n",
    "# print(transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879eaae2",
   "metadata": {},
   "source": [
    "## 3. Stage 2 – Clinical Summarisation\n",
    "\n",
    "Once we have the raw transcript of the call, we summarise it into a concise clinical description using a biomedical language model.  Open‑source transformer models pre‑trained on clinical text include:\n",
    "\n",
    "- **BioBART** (`GanjinZero/biobart-large`) – a BART‑based model fine‑tuned on radiology reports and other biomedical corpora.\n",
    "- **ClinicalT5** – a T5 model adapted to clinical notes.  Some checkpoints require PhysioNet credentials.\n",
    "\n",
    "The following example uses HuggingFace Transformers to load a BioBART model and create a summarisation pipeline.  You can substitute another model name from the HuggingFace Hub if desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a4504bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "What do I think? I think I need an ambulance. I've got a teenager who's turning purple in his face. I think he's soaked out or something.\n"
     ]
    }
   ],
   "source": [
    "# --- Clinical summarisation ---\\n\n",
    "model_name = 'GanjinZero/biobart-large'  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "summariser = pipeline('summarization', model=model, tokenizer=tokenizer)\n",
    "#\\n# # Example transcript (replace with the output of the ASR stage)\n",
    "sample_transcript = (transcript)\n",
    "summary = summariser(sample_transcript, max_length=10, min_length=5)\n",
    "\n",
    "print('Summary:')\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840626ef",
   "metadata": {},
   "source": [
    "## 4. Stage 3 – Stroke Detection\n",
    "\n",
    "In this stage we estimate the likelihood of a stroke based on either speech characteristics or physiological vital signs.  Two example approaches are illustrated:\n",
    "\n",
    "1. **Speech‑based dysarthria detection:** Slurred or impaired speech is a hallmark of stroke.  You can extract acoustic features (e.g. Mel‑frequency cepstral coefficients) from the audio and train a classifier (e.g. SVM, Random Forest) on a dysarthric speech dataset such as TORGO or UASpeech.\n",
    "2. **Vitals‑based classification:** Using vital signs like blood pressure, heart rate and oxygen saturation, train a classifier (e.g. Random Forest) to predict whether a patient has had a stroke.  Suitable training data could come from open ICU datasets (e.g. MIMIC) where stroke diagnosis labels are available.\n",
    "\n",
    "Below are illustrative code snippets showing how to structure these models.  You will need to provide training data and may wish to fine‑tune hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a4c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/asjad99/mimiciii?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10.6M/10.6M [00:00<00:00, 42.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/sraja/.cache/kagglehub/datasets/asjad99/mimiciii/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"asjad99/mimiciii\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b50434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Speech-based dysarthria detection ---\n",
    "# This is a stub demonstrating how you might implement dysarthria (slurred speech) detection\n",
    "# using librosa for feature extraction and scikit-learn for classification.\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def extract_features(audio_path, n_mfcc=13):\n",
    "    \"\"\"Extract MFCC feature vector from an audio file.\"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=16000)\n",
    "    # Compute MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    # Take mean over time axis\n",
    "    return np.mean(mfccs, axis=1)\n",
    "\n",
    "# # Load training data (X_train: list of MFCC feature vectors, y_train: labels 1=dysarthric, 0=healthy)\n",
    "# X_train = ...\n",
    "# y_train = ...\n",
    "\n",
    "# # Train classifier\n",
    "# clf = SVC(kernel='rbf', probability=True)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict stroke likelihood on new audio\n",
    "# X_test = extract_features('call.wav').reshape(1, -1)\n",
    "# prob = clf.predict_proba(X_test)[0, 1]\n",
    "# print(f'Speech-based stroke probability: {prob:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aaa2c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vitals-based stroke classification ---\n",
    "# This demonstrates training a Random Forest on vital sign features to predict stroke.\n",
    "# Replace this stub with your own implementation and dataset.\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Example training data: rows correspond to patients, columns to features like blood pressure (bp),\n",
    "# # heart rate (hr), oxygen saturation (spo2), etc. 'label' indicates whether the patient had a stroke.\n",
    "# # training_data = pd.DataFrame({\n",
    "# #     'bp': [...],\n",
    "# #     'hr': [...],\n",
    "# #     'spo2': [...],\n",
    "# #     'label': [...]\n",
    "# # })\n",
    "\n",
    "# X_train = training_data[['bp', 'hr', 'spo2']]\n",
    "# y_train = training_data['label']\n",
    "\n",
    "# # Train classifier\n",
    "# rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict stroke on new patient vitals\n",
    "# new_vitals = pd.DataFrame({'bp': [170], 'hr': [95], 'spo2': [92]})\n",
    "# prob = rf_clf.predict_proba(new_vitals)[0, 1]\n",
    "# print(f'Vitals-based stroke probability: {prob:.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04ee3c",
   "metadata": {},
   "source": [
    "## 5. Stage 4 – Timeline Generation\n",
    "\n",
    "The final stage stitches together timestamped events from the call transcript, vital signs and interventions into an ordered timeline.  This helps clinicians visualise the patient’s journey from the first call to treatment.  We will use `pandas` to merge and sort events, then display a textual timeline.  You could also generate a Gantt chart using `matplotlib`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de1d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 12:30:45 - 911 call received\n",
      "2025-08-13 12:35:10 - Ambulance dispatched\n",
      "2025-08-13 12:50:00 - Ambulance arrives\n",
      "2025-08-13 12:50:30 - BP 180/95, HR 100, SPO2 93\n",
      "2025-08-13 12:52:00 - BP 185/100, HR 98, SPO2 90\n",
      "2025-08-13 12:55:00 - FAST exam positive\n",
      "2025-08-13 13:10:00 - Arrived at ER\n"
     ]
    }
   ],
   "source": [
    "# --- Timeline generation example ---\n",
    "# Create sample event tables for call, vitals and interventions and merge them into an ordered timeline.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example call events\n",
    "call_events = pd.DataFrame([\n",
    "    {'time': '2025-08-13T12:30:45', 'event': '911 call received'},\n",
    "    {'time': '2025-08-13T12:35:10', 'event': 'Ambulance dispatched'},\n",
    "    {'time': '2025-08-13T12:50:00', 'event': 'Ambulance arrives'},\n",
    "] )\n",
    "\n",
    "# Example vital signs measurements\n",
    "vital_events = pd.DataFrame([\n",
    "    {'time': '2025-08-13T12:50:30', 'event': 'BP 180/95, HR 100, SPO2 93'},\n",
    "    {'time': '2025-08-13T12:52:00', 'event': 'BP 185/100, HR 98, SPO2 90'},\n",
    "] )\n",
    "\n",
    "# Example interventions\n",
    "intervention_events = pd.DataFrame([\n",
    "    {'time': '2025-08-13T12:55:00', 'event': 'FAST exam positive'},\n",
    "    {'time': '2025-08-13T13:10:00', 'event': 'Arrived at ER'},\n",
    "] )\n",
    "\n",
    "# Combine all events\n",
    "events = pd.concat([call_events, vital_events, intervention_events])\n",
    "events['time'] = pd.to_datetime(events['time'])\n",
    "events = events.sort_values('time')\n",
    "\n",
    "# Display timeline\n",
    "for _, row in events.iterrows():\n",
    "    print(f\"{row['time'].strftime('%Y-%m-%d %H:%M:%S')} - {row['event']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2292ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
